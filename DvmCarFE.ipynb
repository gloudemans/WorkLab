{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To Do:\n",
    "* Review and clean up this script\n",
    "* Update training script to save best weights periodically\n",
    "* Update training script to save plots periodically\n",
    "* Update training script to save performance information periodically\n",
    "* Add means to detect training completion (and optionally shut down the instance)\n",
    "* Develop a method to run inference using the weights\n",
    "* Develop an method to visualize the inference results\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Work file /data/dvmcar/dvmcar.zip is already available.\n",
      "Using existing /data/dvmcar\\resized_DVM_v2.zip.\n",
      "Using existing /data/dvmcar\\Confirmed_fronts.zip.\n",
      "Using existing /data/dvmcar\\tables_V2.0.zip.\n",
      "Work file /data/dvmcar/dvmcar.zip is already available.\n",
      "Using existing /data/dvmcar\\resized_DVM_v2.zip.\n",
      "Using existing /data/dvmcar\\Confirmed_fronts.zip.\n",
      "Using existing /data/dvmcar\\tables_V2.0.zip.\n",
      "Training split contains 1161427 images.\n",
      "Validation split contains  145178 images.\n"
     ]
    }
   ],
   "source": [
    "# This cell loads test and training splits.\n",
    "\n",
    "from torchvision import transforms\n",
    "from dvmcar import DvmCarDataset\n",
    "\n",
    "# load_data - return dvmcar training and validation datasets\n",
    "#\n",
    "# locale - specifies environment in which code is exectuting since paths may need\n",
    "#   to be modified.\n",
    "#     \"Lambda Labs\"\n",
    "#     \"Default\"\n",
    "# scale - fraction of dataset to use, 1 uses the entire dataset\n",
    "\n",
    "def load_data(locale=\"Default\", scale=1):\n",
    "\n",
    "    # Depending on the locale...\n",
    "    if locale==\"Lambda Labs\":\n",
    "        # Use lambda labs paths\n",
    "        work_def = '/home/ubuntu/WorkLab/data/dvmcar/dvmcar.zip'\n",
    "        persist_def = '/home/ubuntu/worklab/dvmcar.zip'\n",
    "    else:\n",
    "        # Use default paths\n",
    "        work_def = '/data/dvmcar/dvmcar.zip'\n",
    "        persist_def = None\n",
    "\n",
    "    # Set partitions for train, test, and validate subsets\n",
    "    partition0  = 0.8*scale\n",
    "    partition1  = 0.9*scale\n",
    "    partition2  = 1.0*scale\n",
    "\n",
    "    # Define corresponding split arguments for the dataset constructor\n",
    "    train_split = [0,          partition0]\n",
    "    val_split   = [partition0, partition1]\n",
    "    test_split  = [partition1, partition2]\n",
    "    \n",
    "    # Resnet input height & width?\n",
    "    input_size  = 224\n",
    "\n",
    "    # Specify training transform stack\n",
    "    # Not too sure what random resize crop does...\n",
    "    # Per Derek - maybe color space & other distortions would be useful?\n",
    "    train_transform = transforms.Compose([\n",
    "            transforms.RandomResizedCrop(input_size),\n",
    "            transforms.RandomHorizontalFlip(),\n",
    "            transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225]) ])\n",
    "\n",
    "    # Specify validation transform stack\n",
    "    val_transform = transforms.Compose([\n",
    "            transforms.Resize(input_size),\n",
    "            transforms.CenterCrop(input_size),\n",
    "            transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225]) ])\n",
    "\n",
    "    train_data  = DvmCarDataset(split = train_split, transform = train_transform, work = work_def, persist = persist_def)\n",
    "    val_data    = DvmCarDataset(split =   val_split, transform =   val_transform, work = work_def, persist = persist_def)\n",
    "    \n",
    "    return(train_data, val_data)\n",
    "\n",
    "(train_data, val_data) = load_data()\n",
    "\n",
    "print('Training split contains {:7} images.'.format(len(train_data)))\n",
    "print('Validation split contains {:7} images.'.format(len(val_data)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'ResNet50_Weights' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[1;32mIn [5]\u001b[0m, in \u001b[0;36m<cell line: 355>\u001b[1;34m()\u001b[0m\n\u001b[0;32m    352\u001b[0m         model\u001b[38;5;241m.\u001b[39meval()\n\u001b[0;32m    353\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m model, val_acc_history\n\u001b[1;32m--> 355\u001b[0m manager \u001b[38;5;241m=\u001b[39m \u001b[43mmodel_manager\u001b[49m\u001b[43m(\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_data\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval_data\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "Input \u001b[1;32mIn [5]\u001b[0m, in \u001b[0;36mmodel_manager.__init__\u001b[1;34m(self, datasets)\u001b[0m\n\u001b[0;32m     52\u001b[0m \u001b[38;5;66;03m# Use pretrained model weights\u001b[39;00m\n\u001b[0;32m     53\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39muse_pretrained \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m---> 55\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mimage_size \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minitialize_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     57\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtraining_start \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m     58\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mepoch_start \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n",
      "Input \u001b[1;32mIn [5]\u001b[0m, in \u001b[0;36mmodel_manager.initialize_model\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     72\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel_name \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mresnet\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m     73\u001b[0m     \u001b[38;5;124;03m\"\"\" Resnet50\u001b[39;00m\n\u001b[0;32m     74\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m---> 75\u001b[0m     model \u001b[38;5;241m=\u001b[39m models\u001b[38;5;241m.\u001b[39mresnet50(weights\u001b[38;5;241m=\u001b[39m\u001b[43mResNet50_Weights\u001b[49m\u001b[38;5;241m.\u001b[39mDEFAULT)\n\u001b[0;32m     76\u001b[0m     fc_input_features \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mfc\u001b[38;5;241m.\u001b[39min_features\n\u001b[0;32m     77\u001b[0m     model\u001b[38;5;241m.\u001b[39mfc \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mLinear(fc_input_features, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclasses)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'ResNet50_Weights' is not defined"
     ]
    }
   ],
   "source": [
    "# Import a lot of stuff\n",
    "from IPython.display import display, clear_output\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import os\n",
    "import copy\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "import torchvision\n",
    "from torchvision import models\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "class model_manager:\n",
    "    \n",
    "    def __init__(self, datasets):\n",
    "        \n",
    "        # Training and validation datasets\n",
    "        self.datasets = datasets\n",
    "        \n",
    "        # Output class count\n",
    "        self.classes = datasets[0].classes\n",
    "        \n",
    "        # Maximum number of epochs \n",
    "        self.max_epochs = 100\n",
    "\n",
    "        # Maximum time in minutes\n",
    "        self.max_time = 24*60\n",
    "\n",
    "        # Model save interval (minutes)\n",
    "        self.save_interval = 15\n",
    "\n",
    "        # Flag for feature extracting. When False, we finetune the whole model, \n",
    "        #   when True we only update the reshaped layer params\n",
    "        self.feature_extract = True\n",
    "        \n",
    "        #\n",
    "        self.model_path = \"dvmcar.weights\"\n",
    "\n",
    "        # Models to choose from [resnet, alexnet, vgg, squeezenet, densenet, inception]\n",
    "        self.model_name = \"resnet\"\n",
    "\n",
    "        # Training minibatch size\n",
    "        self.batch_size  = 50\n",
    "\n",
    "        # Shuffle data between epochs\n",
    "        self.shuffle = True \n",
    "        \n",
    "        # Use pretrained model weights\n",
    "        self.use_pretrained = True\n",
    "        \n",
    "        self.model, self.image_size = self.initialize_model()\n",
    "        \n",
    "        self.training_start = 0\n",
    "        self.epoch_start = 0\n",
    "        self.chunk_start = 0\n",
    "        \n",
    "        \n",
    "    # Initialize a pretrained model \n",
    "    #\n",
    "    # model_name - specifies the pretrained model type\n",
    "    # self.classes - specifies the number of output classes\n",
    "\n",
    "    def initialize_model(self):\n",
    "\n",
    "        model = None\n",
    "        image_size = 0\n",
    "\n",
    "        if self.model_name == \"resnet\":\n",
    "            \"\"\" Resnet50\n",
    "            \"\"\"\n",
    "            model = models.resnet50(weights=ResNet50_Weights.DEFAULT)\n",
    "            fc_input_features = model.fc.in_features\n",
    "            model.fc = nn.Linear(fc_input_features, self.classes)\n",
    "            image_size = 224\n",
    "\n",
    "        elif self.model_name == \"alexnet\":\n",
    "            \"\"\" Alexnet\n",
    "            \"\"\"\n",
    "            model = models.alexnet(pretrained=self.use_pretrained)\n",
    "            fc_input_features = model.classifier[6].in_features\n",
    "            model.classifier[6] = nn.Linear(fc_input_features,self.classes)\n",
    "            image_size = 224\n",
    "\n",
    "        elif self.model_name == \"vgg\":\n",
    "            \"\"\" VGG11_bn\n",
    "            \"\"\"\n",
    "            model = models.vgg11_bn(pretrained=self.use_pretrained)\n",
    "            #set_parameter_requires_grad(model, self.feature_extract)\n",
    "            fc_input_features = model.classifier[6].in_features\n",
    "            model.classifier[6] = nn.Linear(fc_input_features,self.classes)\n",
    "            image_size = 224\n",
    "\n",
    "        elif self.model_name == \"squeezenet\":\n",
    "            \"\"\" Squeezenet\n",
    "            \"\"\"\n",
    "            model = models.squeezenet1_0(pretrained=self.use_pretrained)\n",
    "            model.classifier[1] = nn.Conv2d(512, self.classes, kernel_size=(1,1), stride=(1,1))\n",
    "            model.self.classes = self.classes\n",
    "            image_size = 224\n",
    "\n",
    "        elif self.model_name == \"densenet\":\n",
    "            \"\"\" Densenet\n",
    "            \"\"\"\n",
    "            model = models.densenet121(pretrained=self.use_pretrained)\n",
    "            fc_input_features = model.classifier.in_features\n",
    "            model.classifier = nn.Linear(fc_input_features, self.classes) \n",
    "            image_size = 224\n",
    "\n",
    "        elif self.model_name == \"inception\":\n",
    "            \"\"\" Inception v3 \n",
    "            Be careful, expects (299,299) sized images and has auxiliary output\n",
    "            \"\"\"\n",
    "            model = models.inception_v3(pretrained=self.use_pretrained)\n",
    "            # Handle the auxilary net\n",
    "            fc_input_features = model.AuxLogits.fc.in_features\n",
    "            model.AuxLogits.fc = nn.Linear(fc_input_features, self.classes)\n",
    "            # Handle the primary net\n",
    "            fc_input_features = model.fc.in_features\n",
    "            model.fc = nn.Linear(fc_input_features,self.classes)\n",
    "            image_size = 299\n",
    "\n",
    "        else:\n",
    "            print(\"Invalid model name, exiting...\")\n",
    "            exit()\n",
    "            \n",
    "        if self.feature_extract:\n",
    "            for param in model.parameters():\n",
    "                param.requires_grad = False\n",
    "                \n",
    "        return model, image_size\n",
    "    \n",
    "    def log_training_start(self):\n",
    "        now = time.time()\n",
    "        self.training_start = now\n",
    "        print('Started training at {}'.format(now))\n",
    "    \n",
    "    def log_epoch_start(self, epoch):\n",
    "        now = time.time()\n",
    "        self.epoch_start = now\n",
    "        print('Started epoch {}/{} at {}'.format(epoch, self.max_epochs, now))\n",
    "        \n",
    "    def log_chunk_start(self, chunk, chunks):\n",
    "        now = time.time()\n",
    "        self.chunk_start = now\n",
    "        print('Started chunk {}/{} at {}'.format(chunk, chunks, now))\n",
    "    \n",
    "    def log_chunk_complete(self, chunk, chunks):\n",
    "        now = time.time()\n",
    "        print('Completed chunk {}/{} at {} elapsed {}'.format(chunk, chunks, now, now-self.chunk_start))\n",
    "    \n",
    "    def log_epoch_complete(self, epoch):\n",
    "        now = time.time()\n",
    "        print('Completed epoch {}/{} at {} elapsed {}'.format(epoch, self.max_epochs, now, now-self.epoch_start))\n",
    "    \n",
    "    def log_training_complete(self, criteria):\n",
    "        now = time.time()\n",
    "        print('Completed training based on {} at {} elapsed {}'.format(criteria, now, now-self.training_start))\n",
    "        \n",
    "    def update_plots(self, outputs):\n",
    "        \n",
    "        # Get indices (labels) sorting the vector in decreasing order\n",
    "        top_indices = torch.argsort(outputs, 1, descending=True)\n",
    "\n",
    "        # Find predicted ranking of the true class for each result in the batch\n",
    "        n = [(top_indices[k]==labels.data[k]).nonzero().squeeze().item() for k in range(len(labels.data))]\n",
    "\n",
    "        # For each result in the batch...\n",
    "        for idx in n:\n",
    "\n",
    "            # Increment\n",
    "            rank_count[idx] += 1\n",
    "\n",
    "        # Update plot\n",
    "\n",
    "        fig.suptitle('Top N Summary: phase={}, epoch={:3}, sample={:8}'.format(\n",
    "            phase, epoch, sample))\n",
    "\n",
    "        clear_output(wait = True)\n",
    "        x = np.arange(classes)+1\n",
    "\n",
    "        ax1.cla()\n",
    "        ax1.plot(x,          rank_count /sample, label='P(rank==N)')\n",
    "        ax1.plot(x,np.cumsum(rank_count)/sample, label='P(rank in 1..N)')\n",
    "        ax1.set_title('All classes')\n",
    "        ax1.legend()\n",
    "        ax1.grid()\n",
    "        ax1.set_ylabel('Probability')\n",
    "        ax1.set_xlabel('N')    \n",
    "\n",
    "        max_n = 20\n",
    "        ax2.cla()\n",
    "        ax2.plot(x[:max_n],          rank_count[:max_n] /sample, label='P(rank==N)', linestyle='None', marker='o')\n",
    "        ax2.plot(x[:max_n],np.cumsum(rank_count[:max_n])/sample, label='P(rank in 1..N)', linestyle='None', marker='o')\n",
    "        ax2.set_title('Top {} classes'.format(max_n))\n",
    "        ax2.legend()\n",
    "        ax2.grid()\n",
    "        ax2.set_ylabel('Probability')\n",
    "        ax2.set_xlabel('N')    \n",
    "\n",
    "        display(fig)    \n",
    "\n",
    "        #print('Epoch={:3} / Sample={:8}'.format(epoch, sample), end='\\r', flush=True)        \n",
    "    \n",
    "    def train_model(self, dataloaders, criterion, optimizer, is_inception=False):\n",
    "\n",
    "        # Log training start\n",
    "        self.log_training_start()\n",
    "        \n",
    "        # Clear validation accuracy history\n",
    "        val_acc_history = []\n",
    "\n",
    "        # Save best weights\n",
    "        torch.save(self.model.state_dict(), self.model_path)\n",
    "        \n",
    "        # Clear best accuracy\n",
    "        best_acc = 0.0\n",
    "        \n",
    "        # Clear epoch counter\n",
    "        epoch = 0\n",
    "        \n",
    "        # Clear got worse counter\n",
    "        got_worse = 0\n",
    "\n",
    "        # Clear completion flag\n",
    "        training_complete = False\n",
    "        \n",
    "        # While training remains incomplete...\n",
    "        while not training_complete:\n",
    "        \n",
    "            # Log epoch start\n",
    "            self.log_epoch_start(epoch)\n",
    "\n",
    "            # For each phase...\n",
    "            for phase in ['train', 'val']:\n",
    "                \n",
    "                # Depending on the phase\n",
    "                if phase == 'train':\n",
    "                    model.train()  # Set model to training mode\n",
    "                else:\n",
    "                    model.eval()   # Set model to evaluate mode\n",
    "\n",
    "                # Clear accumulators\n",
    "                running_loss = 0.0\n",
    "                running_corrects = 0\n",
    "                sample = 0\n",
    "\n",
    "                # Clear class accumulators\n",
    "                rank_count = np.zeros(classes, dtype=np.float)\n",
    "\n",
    "                # For each minibatch...\n",
    "                for inputs, labels in dataloaders[phase]:\n",
    "                    \n",
    "                    # Move inputs and labels to GPU\n",
    "                    inputs = inputs.to(device)\n",
    "                    labels = labels.to(device)\n",
    "\n",
    "                    # Update sample counter\n",
    "                    sample += inputs.size(0)\n",
    "\n",
    "                    # Clear gradients\n",
    "                    optimizer.zero_grad()\n",
    "\n",
    "                    # forward\n",
    "                    # track history if only in train\n",
    "                    with torch.set_grad_enabled(phase == 'train'):\n",
    "                        # Get model outputs and calculate loss\n",
    "                        # Special case for inception because in training it has an auxiliary output. In train\n",
    "                        #   mode we calculate the loss by summing the final output and the auxiliary output\n",
    "                        #   but in testing we only consider the final output.\n",
    "                        if is_inception and phase == 'train':\n",
    "                            # From https://discuss.pytorch.org/t/how-to-optimize-inception-model-with-auxiliary-classifiers/7958\n",
    "                            outputs, aux_outputs = model(inputs)\n",
    "                            loss1 = criterion(outputs, labels)\n",
    "                            loss2 = criterion(aux_outputs, labels)\n",
    "                            loss = loss1 + 0.4*loss2\n",
    "                        else:\n",
    "                            outputs = model(inputs)\n",
    "                            loss = criterion(outputs, labels)\n",
    "\n",
    "                        # Use maximal class activations as predictions\n",
    "                        _, preds = torch.max(outputs, 1)\n",
    "                        \n",
    "                        self.update_metrics(outputs)\n",
    "\n",
    "                        # backward + optimize only if in training phase\n",
    "                        if phase == 'train':\n",
    "                            loss.backward()\n",
    "                            optimizer.step()\n",
    "\n",
    "                    # statistics\n",
    "                    running_loss += loss.item() * inputs.size(0)\n",
    "                    running_corrects += torch.sum(preds == labels.data)\n",
    "                    \n",
    "                    now = time.time()\n",
    "                    elapsed = now - self.training_start\n",
    "                    \n",
    "                    if elapsed/60 >= self.max_time:\n",
    "                        training_complete = True\n",
    "                        completion_criteria = \"Time\"\n",
    "\n",
    "                epoch_loss = running_loss / len(dataloaders[phase].dataset)\n",
    "                epoch_acc = running_corrects.double() / len(dataloaders[phase].dataset)\n",
    "\n",
    "                print('{} Loss: {:.4f} Acc: {:.4f}'.format(phase, epoch_loss, epoch_acc))\n",
    "\n",
    "                # If validation phase...\n",
    "                if phase == 'val':\n",
    "                \n",
    "                    # Update eopoch accuracy list\n",
    "                    val_acc_history.append(epoch_acc)\n",
    "                    \n",
    "                    # If validation performance improved...\n",
    "                    if epoch_acc > best_acc:\n",
    "                        \n",
    "                        # Update best performance\n",
    "                        best_acc = epoch_acc\n",
    "                        \n",
    "                        # Save best model weights\n",
    "                        torch.save(model.state_dict(), self.model_path)\n",
    "                        \n",
    "                    else:\n",
    "                        got_worse += 1\n",
    "                        if got_worse > 1:\n",
    "                            training_complete = True\n",
    "                            completion_criteria = \"Accuracy\"\n",
    "                            \n",
    "\n",
    "            # Log epoch complete\n",
    "            self.log_epoch_complete(epoch)\n",
    "            \n",
    "            # Increment epoch counter\n",
    "            epoch += 1\n",
    "            \n",
    "            if epoch>=self.max_epochs:\n",
    "                training_complete = True\n",
    "                completion_criteria = \"Epochs\"\n",
    "                \n",
    "        # Report completion\n",
    "        self.log_training_complete(completion_criteria)\n",
    "\n",
    "        #time_elapsed = time.time() - start_time\n",
    "        #print('Training complete in {:.0f}m {:.0f}s'.format(time_elapsed // 60, time_elapsed % 60))\n",
    "        #print('Best val Acc: {:4f}'.format(best_acc))\n",
    "\n",
    "        # load best model weights\n",
    "        model.load_state_dict(best_model_wts)\n",
    "        model.load_state_dict(torch.load(self.model_path))\n",
    "        model.eval()\n",
    "        return model, val_acc_history\n",
    "    \n",
    "manager = model_manager((train_data, val_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'initialize_model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[1;32mIn [3]\u001b[0m, in \u001b[0;36m<cell line: 2>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Initialize the model for this run\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m model_ft, input_size \u001b[38;5;241m=\u001b[39m \u001b[43minitialize_model\u001b[49m(model_name, train_data\u001b[38;5;241m.\u001b[39mclasses, feature_extract, use_pretrained\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m      4\u001b[0m \u001b[38;5;66;03m# Print the model we just instantiated\u001b[39;00m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;66;03m# print(model_ft)\u001b[39;00m\n\u001b[0;32m      7\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInitializing Datasets and Dataloaders...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'initialize_model' is not defined"
     ]
    }
   ],
   "source": [
    "# Initialize the model for this run\n",
    "model_ft, input_size = initialize_model(model_name, train_data.classes, feature_extract, use_pretrained=True)\n",
    "\n",
    "# Print the model we just instantiated\n",
    "# print(model_ft)\n",
    "\n",
    "print(\"Initializing Datasets and Dataloaders...\")\n",
    "\n",
    "# Create training and validation datasets\n",
    "image_datasets = {'train': train_data, 'val' : val_data}\n",
    "\n",
    "# Create training and validation dataloaders\n",
    "dataloaders_dict = {x: torch.utils.data.DataLoader(image_datasets[x], batch_size=batch_size, shuffle=True, num_workers=4) for x in ['train', 'val']}\n",
    "\n",
    "# Detect if we have a GPU available\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Send the model to GPU\n",
    "model_ft = model_ft.to(device)\n",
    "\n",
    "# Gather the parameters to be optimized/updated in this run. If we are\n",
    "#  finetuning we will be updating all parameters. However, if we are \n",
    "#  doing feature extract method, we will only update the parameters\n",
    "#  that we have just initialized, i.e. the parameters with requires_grad\n",
    "#  is True.\n",
    "params_to_update = model_ft.parameters()\n",
    "print(\"Params to learn:\")\n",
    "if feature_extract:\n",
    "    params_to_update = []\n",
    "    for name,param in model_ft.named_parameters():\n",
    "        if param.requires_grad == True:\n",
    "            params_to_update.append(param)\n",
    "            print(\"\\t\",name)\n",
    "else:\n",
    "    for name,param in model_ft.named_parameters():\n",
    "        if param.requires_grad == True:\n",
    "            print(\"\\t\",name)\n",
    "\n",
    "# Observe that all parameters are being optimized\n",
    "optimizer_ft = optim.Adam(params_to_update, lr=0.05)\n",
    "\n",
    "# Setup the loss fxn\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "print(\"Training model...\")\n",
    "\n",
    "# Train and evaluate\n",
    "model_ft, hist = train_model(model_ft, dataloaders_dict, criterion, optimizer_ft, num_epochs=max_epochs, is_inception=(model_name==\"inception\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Writes a csv containing every make and model combination in the dataset\n",
    "def util_list_make_model(dataset):\n",
    "    \n",
    "    maker_list = list(dataset.basic_df['Automaker'])\n",
    "    model_list = list(dataset.basic_df['Genmodel'])\n",
    "\n",
    "    mm_fields = list(set([r[0]+','+r[1] for r in zip(maker_list, model_list)]))\n",
    "    mm_fields.sort()\n",
    "\n",
    "    with open('dvmcars_make_model.csv','w') as out:\n",
    "        out.write('dvmcars-make,dvmcars-model\\n')\n",
    "        for r in mm_fields:\n",
    "            out.write(r+'\\n')\n",
    "            \n",
    "# util_list_make_model(train_data)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
